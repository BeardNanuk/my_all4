#!/bin/bash

# my common ssh command line summary 

## login to cpu node  
(obspy) [jiazeh@paris ~]$ ssh -Y tigercpu
#jiazeh@tigercpu's password: 


## the fold with ssd drive 
[jiazeh@tigercpu ~]$ cd /scratch/gpfs/jiazeh/

## add the following things in ./bashrc under /home/jiazeh
## change the default HOME variable value to ssd addrss 
export HOME="/scratch/gpfs/jiazeh"
cd $HOME
##
# load module on the node is OK, but a lot of times you need to load the modules again
# previous to startign to run specfem2d, especially when switch from GPU to CPU mode, or vise versa
######module load cudatoolkit/9.2 intel/16.0/64/16.0.4.258 openmpi/intel-16.0/1.10.2/64
### for tigercup 
module load openmpi/intel-16.0/1.10.2/64 intel/16.0/64/16.0.4.258

# this is make seisflow (or python in general) will output messages before completing the job 
export PYTHONUNBUFFERED=1

 
## logout of tiger cpu or gpu, to make the bashrc changes take effects 
[jiazeh@tigercpu jiazeh]$ logout
Connection to tigercpu closed.

## look at available modules 
module avail 
# then load the needed ones 
# unload all module 
module purge
# previous to startign to run specfem2d, especially when switch from GPU to CPU mode, or vise versa
module load cudatoolkit/9.2 intel/16.0/64/16.0.4.258 openmpi/intel-16.0/1.10.2/64
# git pull specfem2d 
git clone --recursive --branch devel https://github.com/geodynamics/specfem2d.git
# copy the job submitter for tigercpu (equavalent to launch_specfem.sh)
mv ../parallel ./
# check again to see if all wantted modules have been loaded. 
module list 
#compile with mpi only (will be with cuda for gpu nodes later one)
# clean the related files for old specfem2d installation 
make realclean

# make
make -j
# now xmeshfem2D and xspecfem2D are in the same folder of /bin


### a bit more details on the parallel file 
#!/bin/bash
#SBATCH --nodes=1
#SBATCH -t 0:59:00
#SBATCH --ntasks-per-node=4
##SBATCH --gres=gpu:4

####### my comments for above four lines are here, since # was used as well 
## number of nodes 
## time requested for this jobs. should not be any different when less than any hour 
## number of tasks on one node 
## only useful with gpu, when with cpu node do this: change the following 
#SBATCH --gres=gpu:4
##to 
##SBATCH --gres=gpu:4



cd $HOME/specfem2d/

echo `pwd`
module purge
module load intel/16.0/64/16.0.4.258
module load openmpi/intel-16.0/1.10.2/64
module load cudatoolkit/9.2

module list

NPROC=4
#rm outputmesher
rm OUTPUT_FILES/*
# creates and decomposes mesh
echo
echo "running mesher..."
echo
mpirun -n 1  ./bin/xmeshfem2D >> outputmesher

rm outputsolver*
# runs simulation
echo
echo "  running solver..."
echo

mpirun -np $NPROC ./bin/xspecfem2D >> outputsolver

#mpirun -n $NPROC ./bin/xsmooth_sem 0.002 0.002 Qkappa ./DATA/ ./model T
#mv ./model/proc000000_vp_smooth.bin ./model/proc000000_vp.bin 

echo
echo "see results in directory: OUTPUT_FILES/"
echo
echo "done"
date

 
### related to job submission or queue for information looking 
[jiazeh@tigercpu specfem2d]$ vi parallel 
[jiazeh@tigercpu specfem2d]$ slurmtop
[jiazeh@tigercpu specfem2d]$ slurmtop -u jiazeh
[jiazeh@tigercpu specfem2d]$ squeue -u jiazeh
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
[jiazeh@tigercpu specfem2d]$ scancel -u jiazeh
[jiazeh@tigercpu specfem2d]$ vi parallel 
### change NPROC = 4 to 40 in 'parallel' and also in 'Par_file'  
## this was only for cpu 
## for gpu, for example NPROC will means using for GPUs, and some unknown number of cpu by specfem2d 

[jiazeh@tigercpu specfem2d]$ vi parallel 
[jiazeh@tigercpu specfem2d]$ vi DATA/Par_file 
# submit the job!!!!!!!!!!!!!!!!  
[jiazeh@tigercpu specfem2d]$ sbatch parallel
# all info about the submission of the jobs will be save in slurm-'foonumber'.out
[jiazeh@tigercpu specfem2d]$ cat slurm-757633.out
# so you'd better remove this file before submitting another job 

## do not forget to recompile the specfem2d when doing that again when switching back from GPU mode 
# since, for example, the pml layer open in Par_file would be available only for cpu specfem2d 


### google why the following not working 
[jiazeh@tigercpu specfem2d]$ display OUTPUT_FILES/forward_image0000800.jpg 
'
X11 connection rejected because of wrong authentication.
display: unable to open X server  `localhost:64.0' @ error/display.c/DisplayImageCommand/431.


####################### now gpu ################################ 
## logout of tigercup, then enter tigergpu 
[jiazeh@tigercpu specfem2d]$ logout
Connection to tigercpu closed.
(obspy) [jiazeh@paris ~]$ ssh -Y tigergpu
## by the way, -Y suppose to enable some picture viewing mode 

# recompile and remake on tigergpu 

  95  module avail
   96  module purge
   97  module load
   98  module load openmpi/intel-16.0/1.10.2/64 cudatoolkit/9.2 intel/16.0/64/16.0.4.258
   99  ./configure FC=ifort --with-mpi --with-cuda=cuda8
  100  make realclean
  101  make -j
## vi parallel

#SBATCH --nodes=1
#SBATCH -t 0:59:00
#SBATCH --ntasks-per-node=4
#SBATCH --gres=gpu:4

# the number of tasks on each nodes need to be larger than or equals to the numbr of gpu you requested.
# just ask the number of ntasks as at least four, and the number of gpus also four
  
## NPROC=4
# this determine the number gpu used 

## to submit jobs for gpu 
[jiazeh@tigergpu specfem2d]$ sbatch parallelgpu

## quick copy 
scp ./filenames* jiazeh@tigergpu:/pwd(from gpu current folder)

#### the reason xdg-open not working: 
# .Xauthority is created when using ssh -Y in /home/jiazeh
# but I changed the environmental variable $HOME=/scratch/gpfs/jiazeh as HOME
# so when $HOME/.Xauthority is called, it not exist. 
## solution: 
in home directory : $ cd /scratch/gpfs/jiazeh
$ ln -s /home/jiazeh/.Xauthority .Xauthority 
# after linking, these to should appears the same! 








####### login without keys (password)############
### for local/private machine 
l$
### for remote/public machine 
r$

#open two windows, one private, the other public 
## in home dir of l$ 
# generate private and public keys, keep the (Enter file in which to save the key) and (Enter passphrase ) for empty 
l$ ssh-keygen -t rsa -b 4096
# results are in home/.ssh/
id_rsa, id_rsa.pub, known_hosts 

# create .ssh folder in the real home dir in the remote machine 
r$ mkdir /home/jiazeh/.ssh

# scp the id_rsa.pub file to this folder from local machine 
l$ scp ~/.ssh/id_rsa.pub jiazeh@tigercpu:/scratch/gpfs/jiazeh/.ssh/uploaded_key.pub
# create a file called authorized_keys
r$ cat ~/.ssh/uploaded_key.pub >> /home/jiazeh/.ssh/authorized_keys
# check authorized_keys looks alright
r$ cat authorized_keys 
# set access level to of the folder and files 
r$ chmod 700 /home/jiazeh/.ssh/
r$ chmod 600 /home/jiazeh/.ssh/*
## close and reopen, should be OK


####### 2D inversion on GPU
# sfclean can be run on one node 
# sfrun calls internal seisflows, where 

#### in parameters 

WORKFLOW='inversion'    # inversion, migration
SOLVER='specfem2d_new'      # specfem2d, specfem3d
SYSTEM='tiger_sm_gpu'   # serial, pbs, slurm - $ workflow updated by etienne # in /scratch/gpfs/jiazeh/seisflows/seisflows/system/tiger_sm_gpu.py # used to be 'multicore'
OPTIMIZE='LBFGS'         # base, newton
PREPROCESS='base'       # base
POSTPROCESS='base'      # base
#SCHEME='NLCG'
#$ the line 'GPU_MODE = False' was removed 

MISFIT='Waveform'
MATERIALS='Acoustic'
DENSITY='Constant'
ATTENUATION='no' # this line was added by etienne 

# WORKFLOW
BEGIN=1                # first iteration
END=200
NREC=128
NSRC=4                 # number of sources
SAVEGRADIENT=1        # save gradient how often


# PREPROCESSING
FORMAT='su'   # data file format
CHANNELS='p'            # data channels
NORMALIZE=0             # normalize
BANDPASS=0              # bandpass
MUTE=0                  # mute direct arrival
FREQLO=0.               # low frequency corner
FREQHI=0.               # high frequency corner

MUTECONST=0.            # mute constant
MUTESLOPE=0.            # mute slope
WITH_MPI= True

# POSTPROCESSING
SMOOTH=0
SCALE=6.0e6             # scaling factor
RATIO=0.92
START=1

# OPTIMIZATION
PRECOND=None            # preconditioner type
STEPMAX=15              # maximum trial steps
STEPTHRESH=0.1          # step length safeguard
STEPINIT=0.05


NT=25000         # number of time steps
DT=0.00000002         # time step
F0=500000

# SYSTEM
NTASK=4               # must satisfy 1 <= NTASK <= NSRC
NPROC=1                 # processors per task
NTASKMAX=4            #$ this line was added by etienne, since the NPROCMAX was renamed to NTASKMAX in his version 
NPROCMAX=4
NGPU=4
WALLTIME=1000 # walltime (unit: minute)  ##$ on cluster, this needs to be changed to some reasonable number 

# 


########### slurmp 

## check your jobs
squeue -u jiazeh
























